CS 136 Final Project: Feedback-based Preference Inference in Dating Apps
Charumathi Badrinath, Catherine Cui, Lawrence Zhang

Model: Charu
- n=1000 AGENTS on each side
- each agent has 
    - m=10 ATTRIBUTES, with q=5 OPTIONS for each attribute
    - ATTRIBUTE PREFERENCES over options for each attribute
        - these preferences between agents are correlated (so that ELO makes more sense)
    - randomized weights w_i for each attribute that sum to 1
- each agent scores every other agent based on their attributes and the agent's own attribute
  preferences and weights
    - most preferred option is 5 points, least is 1 point
    - score is sum over all attributes weighted by w_i's
    - total score is from 1 - 5, median score 3
- their full preference profile is the other agents ranked in order of decreasing score

ELO: Lawrence
- Assign each user the same baseline Elo (400, and Elos are non-negative)
- There are x=30 initial rounds
    - in each round, every agent A on each side is paired with a randomly chosen agent B from the other side. Agent B then swipes right or left on agent A. So, agent A's elo can be updated later, at the end of the round.
    - agents swipe right on the agent they are shown with some probability
        - 1/(1 + e^{-2(s - agent's own score))}) probability of swiping right where s is other agent's score from 1 - 5
    - at the end of the round, we update all agents' Elo scores
- There are y=70 rounds
    - do same thing as before except show people with closest ELO
- Get full preference orderings 

ELO update: Lawrence
- Expected score for player A: 1/(1 + 10^{(B - A)/400})
- Expected score for player B: 1/(1 + 10^{(A - B)/400})
- Let S_A = 1 if A wins and S_A = 0 if A loses
- A's new ELO: A_new = A + 32(S_A - E_A)
- Let S_B = 1 if B wins and S_B = 0 if B loses
- B's new ELO: B_new = B + 32(S_B - E_B)

Gale-Shapley: Catherine

Simulation:
- Generate all of the above randomness
- Run Gale-Shapley on the true full preference profile for a baseline matching
- Run Gale-Shapley on the estimated full preference profile generated by Elo for comparison to baseline

# OLD CODE
# 3: Generate matching using Elo, but with more rounds
# global y
# y *= 2
# for agent in agents:
#     # Reset elos
#     agent.elo = starting_elo
# elo2_preference_profiles = elo(agents)
# elo2_gs = gs.GaleShapley(elo2_preference_profiles)
# elo2_gs.match()
# y //= 2
# store_results(3, elo2_gs.matches, true_preference_profiles)

# 4: Generate matching using Elo, but with more skewed global preferences
# global global_preferences
# global_preferences = [16/31, 8/31, 4/31, 2/31, 1/31]
# for agent in agents:
#     agent.elo = starting_elo
# elo3_preference_profiles = elo(agents)
# elo3_gs = gs.GaleShapley(elo3_preference_profiles)
# elo3_gs.match()
# global_preferences = [5/15, 4/15, 3/15, 2/15, 1/15]
# store_results(4, elo3_gs.matches, true_preference_profiles)

# 5: Generate matching using Elo, but with more attributes per agent
# global m
# m *= 2
# for agent in agents:
#     agent.elo = starting_elo
# elo4_preference_profiles = elo(agents)
# elo4_gs = gs.GaleShapley(elo4_preference_profiles)
# elo4_gs.match()
# m /= 2
# store_results(5, elo4_gs.matches, true_preference_profiles)

